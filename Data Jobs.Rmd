---
title: "Data Jobs in Salt Lake City"
output:
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill

---

```{r resources, include=FALSE}

# Packages
library(flexdashboard)
library(tidyverse)
library(rvest)
library(xml2)
library(digest)
library(DT) 
library(stringr)
library(shiny)

# Searches associated with programs

mscm <- c("cybersecurity analyst", "cybersecurity manager", "cybersecurity consultant",
          "chief information security officer", "director of security", "security systems engineer", 
          "security analyst", "security manager", "security auditor", "security architect",
          "security consultant","information security analyst","network security engineer",
          "information security manager","security compliance analyst","penetration tester",
          "vulnerability assessment analyst")

msis <- c("data engineer", "data architect", "cloud engineer", 
           "data warehouse analyst", "data warehouse engineer",
          "data warehouse architect","business intelligence analyst",
           "database administrator", "data integration engineer" )

msba <- c("data analyst", "data scientist")

# empty data frame to return on html parsing error using tryCatch()
error_df <- data.frame(Search = NA,
                   Date = NA, 
                   Title = NA, 
                   Company = NA, 
                   Location = NA, 
                   Link = NA,
                   Description = NA,
                   ID = NA)
```

```{r scraper, include = F}
job_scraper <- function(position = "data analyst"){  # function can take search phrase with up to 4 words, and at least 2
  
  library(rvest)
  library(xml2)
  library(digest)
  library(tidyverse)
  
  # Format search term
  search <- gsub(pattern = " ", replacement = "%20", x = position)
  
  # Format url
  url <- paste0("https://www.indeed.com/jobs?q=",search,"&l=Salt%20Lake%20City%2C%20UT&radius=50&fromage=2")
  
  # Read HTML
  page <- xml2::read_html(url)
                   
  # Get Job Title
  title <- page %>%
    html_nodes('.jobTitle') %>%
    html_nodes('span') %>%
    html_text('title') 
  
  title <- title[title != "new"]
  
  if(length(title)==0){
    
    df <- data.frame(Search = position,
                   Date = NA, 
                   Title = NA, 
                   Company = NA, 
                   Location = NA, 
                   Link = NA,
                   Description = NA,
                   ID = NA)
    
    return(df)

 
     } else {
  
  # Get Company Name
  company <- page %>%
    html_nodes('.companyName') %>%
    html_text('companyName')
  
  # Get Location
  location <- page %>%
    html_nodes('.companyLocation') %>%
    html_text('companyLocation')
  
  # Get Link
  link <- page %>%
    html_nodes('[data-hide-spinner = "true"]') %>%
    html_attr('href')
  
  link <- paste0("https://www.indeed.com", link) 
  
  # Get Job Descriptions
  job_description <- NA # vector for descriptions
    
  # function to get description from each link
  get_description <- function(link){ 
    page <- xml2::read_html(link)
    description <- page %>%
          html_nodes('.jobsearch-jobDescriptionText') %>%
          html_text()
     ifelse(length(description) > 0, description, NA) # sometimes the description appears to be missing. if so, replace w NA
  }
  
  for(j in seq_along(link)) {
   job_description[j] <- get_description(link[j])
   Sys.sleep(1)
 }
  
  # Get date
  date <- Sys.Date()
  
  # Store jobs in data frame and do cleaning
  
  df <- data.frame(Search = rep(position, length(job_description)),
                   Date = date, 
                   Title = title, 
                   Company = company, 
                   Location = location, 
                   Link = link,
                   Description = job_description) %>% 
    mutate(Description = gsub("[\r\n]", "*", Description), # Clean descriptions
           ID = digest(c(Title, Location, Company, Date, Link))) %>% #Create unique ID
    distinct(Link, ID, .keep_all = T) 
  
  return(df)
  
     }
  
  Sys.sleep(rnorm(1, 8, 2)) # random sleep
  
  # Close open connection
   on.exit(close(url))
}
```

```{r read_data, include = F}

# Write empty job_data.csv once to persistent location

# job_data <- data.frame(Search = "", Date =as.Date(""), Title ="", Company ="", Location ="", Description ="", Link = "", ID = "")

# write_csv(job_data, "/opt/app-data/job_data_test.csv")

data <- read_csv("/opt/app-data/job_data.csv")

```

  
```{r extract, include = F}

# length(mscm)
# length(msis)
# length(msbas)

new_data <- data %>%
  bind_rows(tryCatch(job_scraper(mscm[1]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[2]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[3]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[4]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[5]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[6]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[7]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[8]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[9]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[10]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[11]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[12]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[13]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[14]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[15]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[16]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[17]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[1]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[2]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[3]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[4]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[5]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[6]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[7]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[8]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[9]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msba[1]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msba[2]), error = function(x) return(x = error_df))) %>%
  filter(!grepl("United States", Location),
         Company!="CyberCoders") %>%
  distinct(Search, Title, Company, Location, Description, .keep_all = T) %>% 
  select(Search, Date, Title, Company, Location, Description, Link, ID) %>% 
  arrange(desc(Date), Company) %>% 
  na.omit

 write_csv(new_data, "/opt/app-data/job_data.csv")

```



Column {.tabset .tabset-fade}
-------------------------------------

### Open Positions 

```{r}


table_data  <- new_data %>% 
  mutate(Program = ifelse(Search %in% mscm, "MSCM",
                    ifelse(Search %in% msis, "MSIS", "MSBA"))) %>% 
  filter(Date > max(Date) - 60,
         (str_detect(string = tolower(Description), pattern = Search) |
           str_detect(string = tolower(Title), pattern = Search))) %>% # Ensure that search term is actually in the description or the job title
  select(Search, Program, Date, Title, Company, Location, Description, Link, ID) %>% 
  arrange(desc(Date), Company) 
  

table_data$Title <- paste0("<a href='", table_data$Link,"'>", table_data$Title,"</a>")


  
  datatable(select(table_data, -Link, -ID),
              # fillContainer = T,
              escape=F, # This is essential for link display
              extensions = 'Buttons',
              options = list(autoWidth = TRUE,
                             rownames = F,
                             dom = 'Bfrtip',
                             buttons = c('csv', 'excel'),
                             lengthMenu = list(c(10, 25, 50, -1), c(10, 25, 50, "All")),
                             columnDefs = list(
                               list(
                                 targets = c(7), #ie, Description column for hover
                                 render = JS(
                                       "function(data, type, row, meta) {",
                                       "return type === 'display' && data.length > 360 ?",
                                       "'<span title=\"' + data + '\">' + data.substr(0, 360) + '...</span>' : data;",
                                       "}"))),
                             scrollY=F), # This allows for proper  vertical sizing
              callback = JS('table.page(3).draw(false);'),
              caption = "Table includes jobs from the previous 60 days. Hover cursor over Description for full text. Use the search box to find specific job titles or positions  associated with programs. Use the above buttons to download the entire table.")  %>%
  formatStyle(c("Search","Program","Date", "Title", "Company", "Location", "Description"), "vertical-align"="top")




```
   
### Job Search Terms by Program

**MSBA**: 

- data analyst
- data scientist

**MSCM**: 

- cybersecurity analyst
- cybersecurity manager
- cybersecurity consultant
- chief information security officer
- director of security
- security systems engineer
- security analyst
- security manager
- security auditor
- security architect
- security consultant
- information security analyst
- network security engineer
- information security manager
- security compliance analyst
- penetration tester
- vulnerability assessment analyst

**MSIS**: 

- data engineer
- data architect
- cloud engineer
- data warehouse analyst
- data warehouse engineer
- data warehouse architect
- business intelligence analyst
- database administrator
- data integration engineer

### Postings


```{r}
library(ggplot2)

new_data %>% 
  filter(str_detect(string = tolower(Description), pattern = Search) |
           str_detect(string = tolower(Title), pattern = Search)) %>% 
  group_by(Date) %>% 
  count() %>% 
  ggplot(aes(Date, n))+
  geom_line()+
  theme_minimal()+
  labs(title = "Count of Job Postings by Date",
       y = "count",
       caption = "Note: additional search  terms  for MSIS and MSCM programs added on 2/17/22")
  
  
```




### Text Analysis 

```{r}
library(stringr)

new_data$Description <- gsub('[[:punct:]]', ' ', new_data$Description)

# new_data %>% 
#   group_by(Date, ID) %>% 
#   summarize(R = sum(str_detect(string = Description, pattern = " R ")),
#             Python = sum(str_detect(string = Description, pattern = " Python ") |
#                           str_detect(string = Description, pattern = " python ") ),
#             `R only` = ifelse((R > 0 & Python ==0), R, 0),
#             `Python only` = ifelse((R == 0 & Python > 0), Python, 0)) %>% 
#   group_by(Date) %>% 
#   summarize(R = sum(R),
#             Python = sum(Python),
#          `R only` = sum(`R only`),
#          `Python only` = sum(`Python only`)) %>% 
#   mutate(R = cumsum(R),
#             Python = cumsum(Python),
#          `R only` = cumsum(`R only`),
#          `Python only` = cumsum(`Python only`)) %>% 
#   pivot_longer(cols = c("R", "Python", "R only", "Python only"), 
#                names_to = "Language", 
#                values_to = "count") %>% 
#   ggplot(aes(Date, count, col=Language))+
#   geom_line()+
#   theme_minimal()+
#   labs(title = "Cumulative Mentions of R vs. Python in Job Descriptions",
#        y = "count")

new_data %>% 
  filter(str_detect(string = tolower(Description), pattern = Search) |
           str_detect(string = tolower(Title), pattern = Search)) %>% 
  group_by(Date, ID) %>% 
  summarize(R = sum(str_detect(string = Description, pattern = " R ")),
            Python = sum(str_detect(string = Description, pattern = " Python ") |
                          str_detect(string = Description, pattern = " python "))) %>% 
  group_by(Date) %>% 
  summarize(R = sum(R),
            Python = sum(Python)) %>% 
  mutate(R = cumsum(R),
            Python = cumsum(Python)) %>% 
  pivot_longer(cols = c("R", "Python"), 
               names_to = "Language", 
               values_to = "count") %>% 
  ggplot(aes(Date, count, col=Language))+
  geom_line()+
  theme_minimal()+
  labs(title = "Cumulative Mentions of R vs. Python in Job Descriptions",
       y = "count")


```


  
### Job Description Topics

```{r}

```



    





